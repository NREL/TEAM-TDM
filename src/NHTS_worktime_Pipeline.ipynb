{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_battery import *\n",
    "import ml_battery.nhts_data as nhts_data\n",
    "import sklearn.pipeline\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.linear_model\n",
    "import sklearn.cross_validation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip = nhts_data.load_nhts.trip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip[\"real_strttime\"] = (df_trip[\"STRTTIME\"].floordiv(100))*60 + df_trip[\"STRTTIME\"].mod(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_work_trips = df_trip[df_trip[\"WHYFROM\"] == 3].drop(\"WHYFROM\", axis=1)\n",
    "df_to_work_trips = df_trip[df_trip[\"WHYTO\"] == 3].drop(\"WHYTO\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workin_all_day = df_from_work_trips.merge(df_to_work_trips, on=[\"HOUSEID\", \"PERSONID\", \"TDAYDATE\"], suffixes=(\"_fromwork\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_workin_all_day = df_workin_all_day.drop_duplicates(subset=[\"HOUSEID\",\"PERSONID\",\"TDAYDATE\"], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = duplicate_columns(df_unique_workin_all_day)\n",
    "df_unique_workin_all_day_deduped = df_unique_workin_all_day.drop(dups, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(16,12))\n",
    "hist = (df_unique_workin_all_day_deduped[\"real_strttime_fromwork\"] - df_unique_workin_all_day_deduped[\"real_strttime\"]).hist(bins=list(range(-2400,2400,100)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_workin_all_day_deduped[\"work_strttime_endtime_discretized\"] = df_unique_workin_all_day_deduped[\"real_strttime\"].floordiv(60).astype(str) + \"_\" + df_unique_workin_all_day_deduped[\"real_strttime_fromwork\"].floordiv(60).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hh = nhts_data.load_nhts.household()\n",
    "df_per = nhts_data.load_nhts.person()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_unique_workin_all_day_deduped.merge(df_hh).merge(df_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53937, 258)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = duplicate_columns(df)\n",
    "df.drop(dups, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = set(df.columns.values.tolist()) - set([\"STRTTIME\", \"STRTIME_fromwork\", \"real_strttime\", \"real_strttime_fromwork\", \"work_strttime_endtime_discretized\", \"WTTRDFIN\", \"WTHHFIN\", \"WTPERFIN\"])\n",
    "X_columns = sorted(X_columns)\n",
    "\n",
    "y_columns = [\"work_strttime_endtime_discretized\"]\n",
    "\n",
    "weight_column = \"WTTRDFIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df[y_columns[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10 # Anything that occurs less than this will be removed.\n",
    "value_counts = df[y_columns[0]].value_counts()\n",
    "to_remove = value_counts[value_counts <= threshold].index\n",
    "df[y_columns[0]] = df[y_columns[0]].replace(to_remove, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df[y_columns[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = sklearn.cross_validation.train_test_split(df, test_size=0.2, random_state=0, stratify=df[y_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, weights_train = data_train[X_columns], data_train[y_columns], data_train[weight_column]\n",
    "X_test, y_test, weights_test = data_test[X_columns], data_test[y_columns], data_test[weight_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42600, 249)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df[y_columns]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebooks = nhts_data.load_nhts.codebook(sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = codebooks[\"CODEBOOK_HH\"].merge(codebooks[\"CODEBOOK_PER\"], how=\"outer\").merge(codebooks[\"CODEBOOK_TRIP\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of Bicycle Use for Travel C\n"
     ]
    }
   ],
   "source": [
    "codebook = codebook[['Name','Label','Type']]\n",
    "codebook_fromwork = codebook.copy()\n",
    "codebook_fromwork[\"Name\"] += \"_fromwork\"\n",
    "codebook = pd.concat((codebook, codebook_fromwork), ignore_index=True)\n",
    "codebook = codebook.dropna().drop_duplicates().set_index('Name')\n",
    "print(codebook.loc[\"BIKE\"][\"Label\"], codebook.loc[\"BIKE\"][\"Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.pipeline.Pipeline([\n",
    "    ('data_transformation', CodebookTransformer(codebook, X_possible_values = df)),\n",
    "    ('feature_selection', SelectFromModelPandas(sklearn.ensemble.RandomForestClassifier(n_jobs=-1))),\n",
    "    ('classification', StackedEstimator(\n",
    "        MultiEstimator([\n",
    "            (\"random forest\", HyperparameterOptimizedEstimator(sklearn.ensemble.RandomForestClassifier(n_jobs=-1), max_depth=(2,10))),\n",
    "            (\"logistic regression\", sklearn.linear_model.LogisticRegression(C=100000)),\n",
    "            (\"one-hidden mlp, 1000 epochs, optimized hidden\", HyperparameterOptimizedEstimator(OneLayerNNClassifier(n_epochs=1000, regularization=0.01, dropout=1), n_hidden=(1,100))),\n",
    "            #(\"linear SVC\", sklearn.svm.LinearSVC(C=100000)),\n",
    "            #(\"rbf SVC\", sklearn.svm.SVC(kernel=\"rbf\")),\n",
    "            #(\"linear regression (rounded to nearest int)\", IntegerRegressor(sklearn.linear_model.LinearRegression())),\n",
    "            (\"naive bayes\", MixedNB()),\n",
    "            #(\"one-hidden MLP, 1000 epochs, 20 hidden\", OneLayerNNClassifier(20, 1000, batch_size=300, learning_rate=0.001)),\n",
    "            #(\"one-hidden MLP, 1000 epochs, 1 hidden\", OneLayerNNClassifier(1,1000)),\n",
    "            #(\"one-hidden MLP, 10000 epochs, 1 hidden\", OneLayerNNClassifier(1,10000)),\n",
    "            (\"stratified dummy\", PatchedDummy()),\n",
    "            #(\"ordered probit\", OrdinalRegression(\"probit\")),\n",
    "            #(\"nested ordinal logit\", NestedClassifier(sklearn.linear_model.LogisticRegression(C=1000), [0,[1,[2,[3,[4,[5,[6,[7,[8,[9,[10,[11,12]]]]]]]]]]]])),\n",
    "        ], parallel=True),\n",
    "        sklearn.linear_model.LogisticRegression(C=100000)\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USE_PRETRAINED:\n",
    "    with open(\"../models/NHTS_worktime_pipeline.p\",\"rb\") as f:\n",
    "        toy_pipeline = pickle.load(f)\n",
    "    clf=toy_pipeline\n",
    "else:\n",
    "    print(\"fitting!\")\n",
    "    clf.fit(X_train, y_train, classification__sample_weight=weights_train)\n",
    "    with open(\"../models/NHTS_worktime_pipeline.p\",\"wb\") as f:\n",
    "        pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah(*args, **kwargs):\n",
    "    return None, np.zeros((7405, 7405))\n",
    "permutation_importance.get_score_importances = blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "(stacked_scores, stacked_cm, stacked_fi), (multi_scores, multi_cm, multi_fi) = clf.score(X_test, y_test, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy                                          0.592561\n",
       "weighted f1                                       0.576876\n",
       "weighted precision                                0.586613\n",
       "weighted recall                                   0.592561\n",
       "macro f1                                          0.293642\n",
       "macro precision                                   0.338962\n",
       "macro recall                                      0.292311\n",
       "log loss                                         12.839344\n",
       "macro mean absolute market share error          154.544665\n",
       "weighted mean absolute market share error         0.248538\n",
       "training time                                194215.423166\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random forest</th>\n",
       "      <th>logistic regression</th>\n",
       "      <th>one-hidden mlp, 1000 epochs, optimized hidden</th>\n",
       "      <th>naive bayes</th>\n",
       "      <th>stratified dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.211776</td>\n",
       "      <td>0.572185</td>\n",
       "      <td>0.626112</td>\n",
       "      <td>0.259720</td>\n",
       "      <td>0.031537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted f1</th>\n",
       "      <td>0.148739</td>\n",
       "      <td>0.560219</td>\n",
       "      <td>0.598995</td>\n",
       "      <td>0.299503</td>\n",
       "      <td>0.029844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted precision</th>\n",
       "      <td>0.214188</td>\n",
       "      <td>0.571479</td>\n",
       "      <td>0.584566</td>\n",
       "      <td>0.438184</td>\n",
       "      <td>0.028878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted recall</th>\n",
       "      <td>0.211776</td>\n",
       "      <td>0.572185</td>\n",
       "      <td>0.626112</td>\n",
       "      <td>0.259720</td>\n",
       "      <td>0.031537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro f1</th>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.290472</td>\n",
       "      <td>0.251577</td>\n",
       "      <td>0.115194</td>\n",
       "      <td>0.003862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro precision</th>\n",
       "      <td>0.064150</td>\n",
       "      <td>0.333691</td>\n",
       "      <td>0.241867</td>\n",
       "      <td>0.178846</td>\n",
       "      <td>0.003768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro recall</th>\n",
       "      <td>0.024504</td>\n",
       "      <td>0.291570</td>\n",
       "      <td>0.286306</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.004106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log loss</th>\n",
       "      <td>3.656228</td>\n",
       "      <td>3.942233</td>\n",
       "      <td>1.967947</td>\n",
       "      <td>20.362821</td>\n",
       "      <td>33.416672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro mean absolute market share error</th>\n",
       "      <td>211.668822</td>\n",
       "      <td>152.783599</td>\n",
       "      <td>279.702112</td>\n",
       "      <td>1116.606793</td>\n",
       "      <td>250.511011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted mean absolute market share error</th>\n",
       "      <td>1.146497</td>\n",
       "      <td>0.235237</td>\n",
       "      <td>0.348431</td>\n",
       "      <td>0.885585</td>\n",
       "      <td>0.303680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training time</th>\n",
       "      <td>1383.772183</td>\n",
       "      <td>2214.176525</td>\n",
       "      <td>181095.856767</td>\n",
       "      <td>10.358580</td>\n",
       "      <td>1.171413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           random forest  logistic regression  \\\n",
       "accuracy                                        0.211776             0.572185   \n",
       "weighted f1                                     0.148739             0.560219   \n",
       "weighted precision                              0.214188             0.571479   \n",
       "weighted recall                                 0.211776             0.572185   \n",
       "macro f1                                        0.024357             0.290472   \n",
       "macro precision                                 0.064150             0.333691   \n",
       "macro recall                                    0.024504             0.291570   \n",
       "log loss                                        3.656228             3.942233   \n",
       "macro mean absolute market share error        211.668822           152.783599   \n",
       "weighted mean absolute market share error       1.146497             0.235237   \n",
       "training time                                1383.772183          2214.176525   \n",
       "\n",
       "                                           one-hidden mlp, 1000 epochs, optimized hidden  \\\n",
       "accuracy                                                                        0.626112   \n",
       "weighted f1                                                                     0.598995   \n",
       "weighted precision                                                              0.584566   \n",
       "weighted recall                                                                 0.626112   \n",
       "macro f1                                                                        0.251577   \n",
       "macro precision                                                                 0.241867   \n",
       "macro recall                                                                    0.286306   \n",
       "log loss                                                                        1.967947   \n",
       "macro mean absolute market share error                                        279.702112   \n",
       "weighted mean absolute market share error                                       0.348431   \n",
       "training time                                                              181095.856767   \n",
       "\n",
       "                                           naive bayes  stratified dummy  \n",
       "accuracy                                      0.259720          0.031537  \n",
       "weighted f1                                   0.299503          0.029844  \n",
       "weighted precision                            0.438184          0.028878  \n",
       "weighted recall                               0.259720          0.031537  \n",
       "macro f1                                      0.115194          0.003862  \n",
       "macro precision                               0.178846          0.003768  \n",
       "macro recall                                  0.142027          0.004106  \n",
       "log loss                                     20.362821         33.416672  \n",
       "macro mean absolute market share error     1116.606793        250.511011  \n",
       "weighted mean absolute market share error     0.885585          0.303680  \n",
       "training time                                10.358580          1.171413  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('data_transformation', <ml_battery.ml_helpers.CodebookTransformer object at 0x7feb484d96a0>), ('feature_selection', SelectFromModelPandas(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None..., PatchedDummy(constant=None, random_state=None, strategy='stratified'))],\n",
       "        parallel=True)))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_scores[\"stacked model\"] = stacked_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random forest</th>\n",
       "      <th>logistic regression</th>\n",
       "      <th>one-hidden mlp, 1000 epochs, optimized hidden</th>\n",
       "      <th>naive bayes</th>\n",
       "      <th>stratified dummy</th>\n",
       "      <th>stacked model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.211776</td>\n",
       "      <td>0.572185</td>\n",
       "      <td>0.626112</td>\n",
       "      <td>0.259720</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>0.592561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted f1</th>\n",
       "      <td>0.148739</td>\n",
       "      <td>0.560219</td>\n",
       "      <td>0.598995</td>\n",
       "      <td>0.299503</td>\n",
       "      <td>0.029844</td>\n",
       "      <td>0.576876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted precision</th>\n",
       "      <td>0.214188</td>\n",
       "      <td>0.571479</td>\n",
       "      <td>0.584566</td>\n",
       "      <td>0.438184</td>\n",
       "      <td>0.028878</td>\n",
       "      <td>0.586613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted recall</th>\n",
       "      <td>0.211776</td>\n",
       "      <td>0.572185</td>\n",
       "      <td>0.626112</td>\n",
       "      <td>0.259720</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>0.592561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro f1</th>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.290472</td>\n",
       "      <td>0.251577</td>\n",
       "      <td>0.115194</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.293642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro precision</th>\n",
       "      <td>0.064150</td>\n",
       "      <td>0.333691</td>\n",
       "      <td>0.241867</td>\n",
       "      <td>0.178846</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.338962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro recall</th>\n",
       "      <td>0.024504</td>\n",
       "      <td>0.291570</td>\n",
       "      <td>0.286306</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.292311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log loss</th>\n",
       "      <td>3.656228</td>\n",
       "      <td>3.942233</td>\n",
       "      <td>1.967947</td>\n",
       "      <td>20.362821</td>\n",
       "      <td>33.416672</td>\n",
       "      <td>12.839344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro mean absolute market share error</th>\n",
       "      <td>211.668822</td>\n",
       "      <td>152.783599</td>\n",
       "      <td>279.702112</td>\n",
       "      <td>1116.606793</td>\n",
       "      <td>250.511011</td>\n",
       "      <td>154.544665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted mean absolute market share error</th>\n",
       "      <td>1.146497</td>\n",
       "      <td>0.235237</td>\n",
       "      <td>0.348431</td>\n",
       "      <td>0.885585</td>\n",
       "      <td>0.303680</td>\n",
       "      <td>0.248538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training time</th>\n",
       "      <td>1383.772183</td>\n",
       "      <td>2214.176525</td>\n",
       "      <td>181095.856767</td>\n",
       "      <td>10.358580</td>\n",
       "      <td>1.171413</td>\n",
       "      <td>194215.423166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           random forest  logistic regression  \\\n",
       "accuracy                                        0.211776             0.572185   \n",
       "weighted f1                                     0.148739             0.560219   \n",
       "weighted precision                              0.214188             0.571479   \n",
       "weighted recall                                 0.211776             0.572185   \n",
       "macro f1                                        0.024357             0.290472   \n",
       "macro precision                                 0.064150             0.333691   \n",
       "macro recall                                    0.024504             0.291570   \n",
       "log loss                                        3.656228             3.942233   \n",
       "macro mean absolute market share error        211.668822           152.783599   \n",
       "weighted mean absolute market share error       1.146497             0.235237   \n",
       "training time                                1383.772183          2214.176525   \n",
       "\n",
       "                                           one-hidden mlp, 1000 epochs, optimized hidden  \\\n",
       "accuracy                                                                        0.626112   \n",
       "weighted f1                                                                     0.598995   \n",
       "weighted precision                                                              0.584566   \n",
       "weighted recall                                                                 0.626112   \n",
       "macro f1                                                                        0.251577   \n",
       "macro precision                                                                 0.241867   \n",
       "macro recall                                                                    0.286306   \n",
       "log loss                                                                        1.967947   \n",
       "macro mean absolute market share error                                        279.702112   \n",
       "weighted mean absolute market share error                                       0.348431   \n",
       "training time                                                              181095.856767   \n",
       "\n",
       "                                           naive bayes  stratified dummy  \\\n",
       "accuracy                                      0.259720          0.031537   \n",
       "weighted f1                                   0.299503          0.029844   \n",
       "weighted precision                            0.438184          0.028878   \n",
       "weighted recall                               0.259720          0.031537   \n",
       "macro f1                                      0.115194          0.003862   \n",
       "macro precision                               0.178846          0.003768   \n",
       "macro recall                                  0.142027          0.004106   \n",
       "log loss                                     20.362821         33.416672   \n",
       "macro mean absolute market share error     1116.606793        250.511011   \n",
       "weighted mean absolute market share error     0.885585          0.303680   \n",
       "training time                                10.358580          1.171413   \n",
       "\n",
       "                                           stacked model  \n",
       "accuracy                                        0.592561  \n",
       "weighted f1                                     0.576876  \n",
       "weighted precision                              0.586613  \n",
       "weighted recall                                 0.592561  \n",
       "macro f1                                        0.293642  \n",
       "macro precision                                 0.338962  \n",
       "macro recall                                    0.292311  \n",
       "log loss                                       12.839344  \n",
       "macro mean absolute market share error        154.544665  \n",
       "weighted mean absolute market share error       0.248538  \n",
       "training time                              194215.423166  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.steps[-1][1].multiestimator.estimators[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} &  random forest &  logistic regression &  one-hidden mlp, 1000 epochs, optimized hidden &  naive bayes &  stratified dummy &  stacked model \\\\\n",
      "\\midrule\n",
      "accuracy                                  &          0.212 &                0.572 &                                          0.626 &        0.260 &             0.032 &          0.593 \\\\\n",
      "weighted f1                               &          0.149 &                0.560 &                                          0.599 &        0.300 &             0.030 &          0.577 \\\\\n",
      "weighted precision                        &          0.214 &                0.571 &                                          0.585 &        0.438 &             0.029 &          0.587 \\\\\n",
      "weighted recall                           &          0.212 &                0.572 &                                          0.626 &        0.260 &             0.032 &          0.593 \\\\\n",
      "macro f1                                  &          0.024 &                0.290 &                                          0.252 &        0.115 &             0.004 &          0.294 \\\\\n",
      "macro precision                           &          0.064 &                0.334 &                                          0.242 &        0.179 &             0.004 &          0.339 \\\\\n",
      "macro recall                              &          0.025 &                0.292 &                                          0.286 &        0.142 &             0.004 &          0.292 \\\\\n",
      "log loss                                  &          3.656 &                3.942 &                                          1.968 &       20.363 &            33.417 &         12.839 \\\\\n",
      "macro mean absolute market share error    &        211.669 &              152.784 &                                        279.702 &     1116.607 &           250.511 &        154.545 \\\\\n",
      "weighted mean absolute market share error &          1.146 &                0.235 &                                          0.348 &        0.886 &             0.304 &          0.249 \\\\\n",
      "training time                             &       1383.772 &             2214.177 &                                     181095.857 &       10.359 &             1.171 &     194215.423 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(multi_scores.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "(stacked_scores, stacked_cm, stacked_fi), (multi_scores, multi_cm, multi_fi) = clf.score(X_train, y_train, sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random forest</th>\n",
       "      <th>logistic regression</th>\n",
       "      <th>one-hidden mlp, 1000 epochs, optimized hidden</th>\n",
       "      <th>naive bayes</th>\n",
       "      <th>stratified dummy</th>\n",
       "      <th>stacked model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.323287</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.856029</td>\n",
       "      <td>0.583917</td>\n",
       "      <td>0.026442</td>\n",
       "      <td>0.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted f1</th>\n",
       "      <td>0.294197</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.830858</td>\n",
       "      <td>0.647869</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.926720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted precision</th>\n",
       "      <td>0.564038</td>\n",
       "      <td>0.997291</td>\n",
       "      <td>0.810761</td>\n",
       "      <td>0.811084</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>0.927702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted recall</th>\n",
       "      <td>0.323287</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.856029</td>\n",
       "      <td>0.583917</td>\n",
       "      <td>0.026442</td>\n",
       "      <td>0.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro f1</th>\n",
       "      <td>0.154711</td>\n",
       "      <td>0.999743</td>\n",
       "      <td>0.442546</td>\n",
       "      <td>0.713864</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.973588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro precision</th>\n",
       "      <td>0.471149</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.440443</td>\n",
       "      <td>0.770500</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.982995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro recall</th>\n",
       "      <td>0.114168</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.463950</td>\n",
       "      <td>0.781272</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.966222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log loss</th>\n",
       "      <td>3.562805</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>0.974731</td>\n",
       "      <td>15.674556</td>\n",
       "      <td>33.457210</td>\n",
       "      <td>3.724675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro mean absolute market share error</th>\n",
       "      <td>195.182288</td>\n",
       "      <td>0.031994</td>\n",
       "      <td>433.007010</td>\n",
       "      <td>488.696136</td>\n",
       "      <td>64.182500</td>\n",
       "      <td>5.976942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted mean absolute market share error</th>\n",
       "      <td>1.047944</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.268735</td>\n",
       "      <td>0.600251</td>\n",
       "      <td>0.109195</td>\n",
       "      <td>0.026451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training time</th>\n",
       "      <td>1383.772183</td>\n",
       "      <td>2214.176525</td>\n",
       "      <td>181095.856767</td>\n",
       "      <td>10.358580</td>\n",
       "      <td>1.171413</td>\n",
       "      <td>194215.423166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           random forest  logistic regression  \\\n",
       "accuracy                                        0.323287             0.997290   \n",
       "weighted f1                                     0.294197             0.997290   \n",
       "weighted precision                              0.564038             0.997291   \n",
       "weighted recall                                 0.323287             0.997290   \n",
       "macro f1                                        0.154711             0.999743   \n",
       "macro precision                                 0.471149             0.999752   \n",
       "macro recall                                    0.114168             0.999735   \n",
       "log loss                                        3.562805             0.089629   \n",
       "macro mean absolute market share error        195.182288             0.031994   \n",
       "weighted mean absolute market share error       1.047944             0.001042   \n",
       "training time                                1383.772183          2214.176525   \n",
       "\n",
       "                                           one-hidden mlp, 1000 epochs, optimized hidden  \\\n",
       "accuracy                                                                        0.856029   \n",
       "weighted f1                                                                     0.830858   \n",
       "weighted precision                                                              0.810761   \n",
       "weighted recall                                                                 0.856029   \n",
       "macro f1                                                                        0.442546   \n",
       "macro precision                                                                 0.440443   \n",
       "macro recall                                                                    0.463950   \n",
       "log loss                                                                        0.974731   \n",
       "macro mean absolute market share error                                        433.007010   \n",
       "weighted mean absolute market share error                                       0.268735   \n",
       "training time                                                              181095.856767   \n",
       "\n",
       "                                           naive bayes  stratified dummy  \\\n",
       "accuracy                                      0.583917          0.026442   \n",
       "weighted f1                                   0.647869          0.026400   \n",
       "weighted precision                            0.811084          0.026421   \n",
       "weighted recall                               0.583917          0.026442   \n",
       "macro f1                                      0.713864          0.003411   \n",
       "macro precision                               0.770500          0.003423   \n",
       "macro recall                                  0.781272          0.003413   \n",
       "log loss                                     15.674556         33.457210   \n",
       "macro mean absolute market share error      488.696136         64.182500   \n",
       "weighted mean absolute market share error     0.600251          0.109195   \n",
       "training time                                10.358580          1.171413   \n",
       "\n",
       "                                           stacked model  \n",
       "accuracy                                        0.926800  \n",
       "weighted f1                                     0.926720  \n",
       "weighted precision                              0.927702  \n",
       "weighted recall                                 0.926800  \n",
       "macro f1                                        0.973588  \n",
       "macro precision                                 0.982995  \n",
       "macro recall                                    0.966222  \n",
       "log loss                                        3.724675  \n",
       "macro mean absolute market share error          5.976942  \n",
       "weighted mean absolute market share error       0.026451  \n",
       "training time                              194215.423166  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_scores[\"stacked model\"] = stacked_scores\n",
    "multi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19080836955.679348"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cm[\"random forest\"].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42600, 249)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('one-hidden mlp, 1000 epochs, optimized hidden',\n",
       " OneLayerNNClassifier(batch_size=None, dropout=1, learning_rate=0.01,\n",
       "            n_epochs=1000, n_hidden=20, regularization=0.01, trainable=True))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.steps[-1][1].multiestimator.estimators[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = clf.steps[1][1].estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "col = clf.steps[0][1].transform(X_test).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=list(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidf = pd.DataFrame(fi, index=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DWELTIME</th>\n",
       "      <td>0.008617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCDWORK</th>\n",
       "      <td>0.005467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_AGE</th>\n",
       "      <td>0.005444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRPMILES</th>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTTOWK17</th>\n",
       "      <td>0.005307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRVLCMIN</th>\n",
       "      <td>0.005236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRPMILES_fromwork</th>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VMT_MILE</th>\n",
       "      <td>0.005081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIMETOWK</th>\n",
       "      <td>0.005005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRVLCMIN_fromwork</th>\n",
       "      <td>0.004916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VMT_MILE_fromwork</th>\n",
       "      <td>0.004863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNTTDHH</th>\n",
       "      <td>0.004785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOCONG</th>\n",
       "      <td>0.004679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARMILE</th>\n",
       "      <td>0.004537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NWALKTRP</th>\n",
       "      <td>0.004252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALK4EX</th>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELIVER</th>\n",
       "      <td>0.004070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DWELTIME_fromwork</th>\n",
       "      <td>0.003852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPACT</th>\n",
       "      <td>0.003737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNTTDTR</th>\n",
       "      <td>0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHVEHCNT</th>\n",
       "      <td>0.003505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESP_CNT</th>\n",
       "      <td>0.003260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STRTTIME_fromwork_____1700</th>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRKTIME_____08:00 AM</th>\n",
       "      <td>0.002978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRKCOUNT</th>\n",
       "      <td>0.002973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPACT</th>\n",
       "      <td>0.002850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRVRCNT</th>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEALTH_____2</th>\n",
       "      <td>0.002516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLACE_____3</th>\n",
       "      <td>0.002503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUMADLT</th>\n",
       "      <td>0.002490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303904900105</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEID_____30290679</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910770301</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910770201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910770101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910370201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910360101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910250101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910130201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910100301</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303910100203</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEID_____30290675</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303909040201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303908940101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303908720201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303908720104</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303907840101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303907200201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303907190101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303906900201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303906550101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303906230101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905870102</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905830201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905690201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905690101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905520201</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905470302</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____303905470101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDCASEID_____400718670101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153689 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0\n",
       "DWELTIME                    0.008617\n",
       "GCDWORK                     0.005467\n",
       "R_AGE                       0.005444\n",
       "TRPMILES                    0.005437\n",
       "DISTTOWK17                  0.005307\n",
       "TRVLCMIN                    0.005236\n",
       "TRPMILES_fromwork           0.005212\n",
       "VMT_MILE                    0.005081\n",
       "TIMETOWK                    0.005005\n",
       "TRVLCMIN_fromwork           0.004916\n",
       "VMT_MILE_fromwork           0.004863\n",
       "CNTTDHH                     0.004785\n",
       "NOCONG                      0.004679\n",
       "YEARMILE                    0.004537\n",
       "NWALKTRP                    0.004252\n",
       "WALK4EX                     0.004149\n",
       "DELIVER                     0.004070\n",
       "DWELTIME_fromwork           0.003852\n",
       "LPACT                       0.003737\n",
       "CNTTDTR                     0.003641\n",
       "HHVEHCNT                    0.003505\n",
       "RESP_CNT                    0.003260\n",
       "STRTTIME_fromwork_____1700  0.003000\n",
       "WRKTIME_____08:00 AM        0.002978\n",
       "WRKCOUNT                    0.002973\n",
       "VPACT                       0.002850\n",
       "DRVRCNT                     0.002740\n",
       "HEALTH_____2                0.002516\n",
       "PLACE_____3                 0.002503\n",
       "NUMADLT                     0.002490\n",
       "...                              ...\n",
       "TDCASEID_____303904900105   0.000000\n",
       "HOUSEID_____30290679        0.000000\n",
       "TDCASEID_____303910770301   0.000000\n",
       "TDCASEID_____303910770201   0.000000\n",
       "TDCASEID_____303910770101   0.000000\n",
       "TDCASEID_____303910370201   0.000000\n",
       "TDCASEID_____303910360101   0.000000\n",
       "TDCASEID_____303910250101   0.000000\n",
       "TDCASEID_____303910130201   0.000000\n",
       "TDCASEID_____303910100301   0.000000\n",
       "TDCASEID_____303910100203   0.000000\n",
       "HOUSEID_____30290675        0.000000\n",
       "TDCASEID_____303909040201   0.000000\n",
       "TDCASEID_____303908940101   0.000000\n",
       "TDCASEID_____303908720201   0.000000\n",
       "TDCASEID_____303908720104   0.000000\n",
       "TDCASEID_____303907840101   0.000000\n",
       "TDCASEID_____303907200201   0.000000\n",
       "TDCASEID_____303907190101   0.000000\n",
       "TDCASEID_____303906900201   0.000000\n",
       "TDCASEID_____303906550101   0.000000\n",
       "TDCASEID_____303906230101   0.000000\n",
       "TDCASEID_____303905870102   0.000000\n",
       "TDCASEID_____303905830201   0.000000\n",
       "TDCASEID_____303905690201   0.000000\n",
       "TDCASEID_____303905690101   0.000000\n",
       "TDCASEID_____303905520201   0.000000\n",
       "TDCASEID_____303905470302   0.000000\n",
       "TDCASEID_____303905470101   0.000000\n",
       "TDCASEID_____400718670101   0.000000\n",
       "\n",
       "[153689 rows x 1 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fidf.sort_values(0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on the random forest thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.base\n",
    "import types\n",
    "import copy\n",
    "def fake_fit(self, *args, **kwargs):\n",
    "    return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_est = copy.deepcopy(clf.steps[1][1].estimator_)\n",
    "rf_est.fit = types.MethodType(fake_fit, rf_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fs_pipe = sklearn.pipeline.Pipeline([\n",
    "    copy.deepcopy(clf.steps[0]),\n",
    "    (\"classification\", MultiEstimator([(\"rf\", rf_est)]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/handlers.py\", line 633, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/handlers.py\", line 600, in makePickle\n",
      "    d['msg'] = record.getMessage()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-53-c9c5d69894a1>\", line 1, in <module>\n",
      "    rf_fs_pipe.fit(X_train, y_train, classification__sample_weight=weights_train)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/rufous/occupancy_detection/t7_travel_demand/src/ml_battery/stacked_estimators.py\", line 59, in fit\n",
      "    log.info(\"opened sessions: \", n_opened_sessions())\n",
      "Message: 'opened sessions: '\n",
      "Arguments: ('0/0',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('data_transformation', <ml_battery.ml_helpers.CodebookTransformer object at 0x7feb4856dfd0>), ('classification', MultiEstimator(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,...core=False, random_state=None, verbose=0,\n",
       "            warm_start=False))],\n",
       "        parallel=False))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_fs_pipe.fit(X_train, y_train, classification__sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah(*args, **kwargs):\n",
    "    return None, np.zeros((153689, 153689))\n",
    "permutation_importance.get_score_importances = blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                 rf\n",
       " accuracy                                   0.998241\n",
       " weighted f1                                0.998238\n",
       " weighted precision                         0.998249\n",
       " weighted recall                            0.998241\n",
       " macro f1                                   0.999251\n",
       " macro precision                            0.999536\n",
       " macro recall                               0.998982\n",
       " log loss                                   0.430898\n",
       " macro mean absolute market share error     0.309250\n",
       " weighted mean absolute market share error  0.002544\n",
       " training time                              0.000010,\n",
       " {'rf':               10_10         10_11         10_12         10_13         10_14  \\\n",
       "  10_10  4.850457e+06  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_11  0.000000e+00  2.432278e+07  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_12  0.000000e+00  0.000000e+00  3.709265e+07  0.000000e+00  0.000000e+00   \n",
       "  10_13  0.000000e+00  0.000000e+00  0.000000e+00  4.053757e+07  0.000000e+00   \n",
       "  10_14  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  4.268330e+07   \n",
       "  10_15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_16  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_17  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_18  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_20  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_14  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_16  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_17  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_18  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_20  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  ...             ...           ...           ...           ...           ...   \n",
       "  8_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_14   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_16   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_17   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_18   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_19   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_20   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_21   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_22   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_8    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_14   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_16   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_17   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_18   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_19   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_20   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_21   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_22   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_23   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  \n",
       "                10_15         10_16         10_17         10_18         10_19  \\\n",
       "  10_10  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_14  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_15  5.540056e+07  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_16  0.000000e+00  7.914632e+07  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_17  0.000000e+00  0.000000e+00  1.117347e+08  0.000000e+00  0.000000e+00   \n",
       "  10_18  0.000000e+00  0.000000e+00  0.000000e+00  9.819065e+07  0.000000e+00   \n",
       "  10_19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  6.904437e+07   \n",
       "  10_20  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_14  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_16  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_17  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_18  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_20  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  ...             ...           ...           ...           ...           ...   \n",
       "  8_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_14   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_16   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_17   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_18   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_19   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_20   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_21   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_22   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_8    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_14   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_16   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_17   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_18   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_19   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_20   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_21   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_22   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_23   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  \n",
       "             ...               9_15          9_16          9_17          9_18  \\\n",
       "  10_10      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_11      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_12      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_13      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_14      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_15      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_16      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_17      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_18      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_19      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_20      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_21      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_22      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_23      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_11      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_12      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_13      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_14      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_15      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_16      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_17      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_18      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_19      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_20      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_21      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_22      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_23      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_11      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_12      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_13      ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  ...        ...                ...           ...           ...           ...   \n",
       "  8_10       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_11       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_12       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_13       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_14       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_15       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_16       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_17       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_18       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_19       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_20       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_21       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_22       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_8        ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_9        ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_10       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_11       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_12       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_13       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_14       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_15       ...       9.953528e+07  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_16       ...       0.000000e+00  1.553856e+08  0.000000e+00  0.000000e+00   \n",
       "  9_17       ...       0.000000e+00  0.000000e+00  2.760030e+08  0.000000e+00   \n",
       "  9_18       ...       0.000000e+00  0.000000e+00  0.000000e+00  2.521521e+08   \n",
       "  9_19       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_20       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_21       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_22       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_23       ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_9        ...       0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  \n",
       "                 9_19          9_20          9_21          9_22          9_23  \\\n",
       "  10_10  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_14  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_16  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_17  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_18  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_20  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_14  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_16  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_17  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_18  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_20  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  ...             ...           ...           ...           ...           ...   \n",
       "  8_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_14   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_16   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_17   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_18   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_19   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_20   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_21   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_22   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_8    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_14   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_16   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_17   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_18   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_19   1.379820e+08  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_20   0.000000e+00  8.488624e+07  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_21   0.000000e+00  0.000000e+00  3.245079e+07  0.000000e+00  0.000000e+00   \n",
       "  9_22   0.000000e+00  0.000000e+00  0.000000e+00  2.736093e+07  0.000000e+00   \n",
       "  9_23   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.007346e+07   \n",
       "  9_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  \n",
       "                  9_9  \n",
       "  10_10  0.000000e+00  \n",
       "  10_11  0.000000e+00  \n",
       "  10_12  0.000000e+00  \n",
       "  10_13  0.000000e+00  \n",
       "  10_14  0.000000e+00  \n",
       "  10_15  0.000000e+00  \n",
       "  10_16  0.000000e+00  \n",
       "  10_17  0.000000e+00  \n",
       "  10_18  0.000000e+00  \n",
       "  10_19  0.000000e+00  \n",
       "  10_20  0.000000e+00  \n",
       "  10_21  0.000000e+00  \n",
       "  10_22  0.000000e+00  \n",
       "  10_23  0.000000e+00  \n",
       "  11_11  0.000000e+00  \n",
       "  11_12  0.000000e+00  \n",
       "  11_13  0.000000e+00  \n",
       "  11_14  0.000000e+00  \n",
       "  11_15  0.000000e+00  \n",
       "  11_16  0.000000e+00  \n",
       "  11_17  0.000000e+00  \n",
       "  11_18  0.000000e+00  \n",
       "  11_19  0.000000e+00  \n",
       "  11_20  0.000000e+00  \n",
       "  11_21  0.000000e+00  \n",
       "  11_22  0.000000e+00  \n",
       "  11_23  0.000000e+00  \n",
       "  12_11  0.000000e+00  \n",
       "  12_12  0.000000e+00  \n",
       "  12_13  0.000000e+00  \n",
       "  ...             ...  \n",
       "  8_10   0.000000e+00  \n",
       "  8_11   0.000000e+00  \n",
       "  8_12   0.000000e+00  \n",
       "  8_13   0.000000e+00  \n",
       "  8_14   0.000000e+00  \n",
       "  8_15   0.000000e+00  \n",
       "  8_16   0.000000e+00  \n",
       "  8_17   0.000000e+00  \n",
       "  8_18   0.000000e+00  \n",
       "  8_19   0.000000e+00  \n",
       "  8_20   0.000000e+00  \n",
       "  8_21   0.000000e+00  \n",
       "  8_22   0.000000e+00  \n",
       "  8_8    0.000000e+00  \n",
       "  8_9    0.000000e+00  \n",
       "  9_10   0.000000e+00  \n",
       "  9_11   0.000000e+00  \n",
       "  9_12   0.000000e+00  \n",
       "  9_13   0.000000e+00  \n",
       "  9_14   0.000000e+00  \n",
       "  9_15   0.000000e+00  \n",
       "  9_16   0.000000e+00  \n",
       "  9_17   0.000000e+00  \n",
       "  9_18   0.000000e+00  \n",
       "  9_19   0.000000e+00  \n",
       "  9_20   0.000000e+00  \n",
       "  9_21   0.000000e+00  \n",
       "  9_22   0.000000e+00  \n",
       "  9_23   0.000000e+00  \n",
       "  9_9    9.758094e+06  \n",
       "  \n",
       "  [222 rows x 222 columns]},\n",
       "                     rf\n",
       " BIKE4EX            0.0\n",
       " BIKESHARE          0.0\n",
       " CARRODE            0.0\n",
       " CARSHARE           0.0\n",
       " CNTTDHH            0.0\n",
       " CNTTDTR            0.0\n",
       " DELIVER            0.0\n",
       " DISTTOSC17         0.0\n",
       " DISTTOWK17         0.0\n",
       " DRVRCNT            0.0\n",
       " DWELTIME           0.0\n",
       " DWELTIME_fromwork  0.0\n",
       " GCDWORK            0.0\n",
       " HHVEHCNT           0.0\n",
       " HH_ONTD            0.0\n",
       " HH_ONTD_fromwork   0.0\n",
       " LPACT              0.0\n",
       " MCUSED             0.0\n",
       " NBIKETRP           0.0\n",
       " NOCONG             0.0\n",
       " NONHHCNT           0.0\n",
       " NONHHCNT_fromwork  0.0\n",
       " NUMADLT            0.0\n",
       " NUMONTRP           0.0\n",
       " NUMONTRP_fromwork  0.0\n",
       " NUMTRANS           0.0\n",
       " NUMTRANS_fromwork  0.0\n",
       " NWALKTRP           0.0\n",
       " PTUSED             0.0\n",
       " PUBTIME            0.0\n",
       " ...                ...\n",
       " WRK_HOME_____-9    0.0\n",
       " WRK_HOME_____1     0.0\n",
       " WRK_HOME_____2     0.0\n",
       " W_CANE_____-1      0.0\n",
       " W_CANE_____-9      0.0\n",
       " W_CANE_____1       0.0\n",
       " W_CHAIR_____-1     0.0\n",
       " W_CHAIR_____-9     0.0\n",
       " W_CHAIR_____7      0.0\n",
       " W_CRUTCH_____-1    0.0\n",
       " W_CRUTCH_____-9    0.0\n",
       " W_CRUTCH_____5     0.0\n",
       " W_DOG_____-1       0.0\n",
       " W_DOG_____-9       0.0\n",
       " W_DOG_____4        0.0\n",
       " W_MTRCHR_____-1    0.0\n",
       " W_MTRCHR_____-9    0.0\n",
       " W_MTRCHR_____8     0.0\n",
       " W_NONE_____-1      0.0\n",
       " W_NONE_____-9      0.0\n",
       " W_NONE_____0       0.0\n",
       " W_SCOOTR_____-1    0.0\n",
       " W_SCOOTR_____-9    0.0\n",
       " W_SCOOTR_____6     0.0\n",
       " W_WHCANE_____-1    0.0\n",
       " W_WHCANE_____-9    0.0\n",
       " W_WHCANE_____3     0.0\n",
       " W_WLKR_____-1      0.0\n",
       " W_WLKR_____-9      0.0\n",
       " W_WLKR_____2       0.0\n",
       " \n",
       " [153689 rows x 1 columns])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_fs_pipe.score(X_train, y_train, sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2_pipe = sklearn.pipeline.Pipeline(\n",
    "    copy.deepcopy(clf.steps[:2]) +\n",
    "    [(\"classification\", MultiEstimator(copy.deepcopy(clf.steps[-1][1].multiestimator.estimators[:1])))\n",
    "        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf2_pipe.steps[-1][1].estimators[0][1].fit = types.MethodType(fake_fit, rf2_pipe.steps[-1][1].estimators[0][1])\n",
    "rf2_pipe.steps[-1][1].estimators[0][1].max_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/feature_selection/from_model.py:169: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/handlers.py\", line 633, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/handlers.py\", line 600, in makePickle\n",
      "    d['msg'] = record.getMessage()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-64-720f52a931c0>\", line 1, in <module>\n",
      "    rf2_pipe.fit(X_train, y_train, classification__sample_weight=weights_train)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/rufous/occupancy_detection/t7_travel_demand/src/ml_battery/stacked_estimators.py\", line 59, in fit\n",
      "    log.info(\"opened sessions: \", n_opened_sessions())\n",
      "Message: 'opened sessions: '\n",
      "Arguments: ('0/0',)\n",
      "/rufous/occupancy_detection/t7_travel_demand/src/ml_battery/stacked_estimators.py:61: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimators[i] = (name, estimator.fit(X,y,**fit_params)) # in case of hyperparameter optimized thingies, this returns the underlying estimator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('data_transformation', <ml_battery.ml_helpers.CodebookTransformer object at 0x7feb155850b8>), ('feature_selection', SelectFromModelPandas(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None...core=False, random_state=None, verbose=0,\n",
       "            warm_start=False))],\n",
       "        parallel=False))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2_pipe.fit(X_train, y_train, classification__sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah(*args, **kwargs):\n",
    "    return None, np.zeros((7433, 7433))\n",
    "permutation_importance.get_score_importances = blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                           random forest\n",
       " accuracy                                        0.161966\n",
       " weighted f1                                     0.091026\n",
       " weighted precision                              0.156675\n",
       " weighted recall                                 0.161966\n",
       " macro f1                                        0.008103\n",
       " macro precision                                 0.023828\n",
       " macro recall                                    0.011948\n",
       " log loss                                        3.947221\n",
       " macro mean absolute market share error        226.188731\n",
       " weighted mean absolute market share error       1.522335\n",
       " training time                                   0.914737,\n",
       " {'random forest':        10_10  10_11  10_12  10_13  10_14          10_15  10_16  10_17  10_18  \\\n",
       "  10_10    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_11    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_12    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_13    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_14    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_15    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_16    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_17    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_18    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_19    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_20    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_21    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_22    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  10_23    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_11    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_12    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_13    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_14    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_15    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_16    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_17    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_18    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_19    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_20    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_21    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_22    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  11_23    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  12_11    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  12_12    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  12_13    0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  ...      ...    ...    ...    ...    ...            ...    ...    ...    ...   \n",
       "  8_10     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_11     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_12     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_13     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_14     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_15     0.0    0.0    0.0    0.0    0.0  107866.564075    0.0    0.0    0.0   \n",
       "  8_16     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_17     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_18     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_19     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_20     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_21     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_22     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_8      0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  8_9      0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_10     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_11     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_12     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_13     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_14     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_15     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_16     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_17     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_18     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_19     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_20     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_21     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_22     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_23     0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  9_9      0.0    0.0    0.0    0.0    0.0       0.000000    0.0    0.0    0.0   \n",
       "  \n",
       "         10_19 ...   9_15  9_16  9_17  9_18  9_19  9_20  9_21  9_22  9_23  9_9  \n",
       "  10_10    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_11    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_12    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_13    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_14    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_15    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_16    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_17    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_18    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_19    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_20    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_21    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_22    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  10_23    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_11    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_12    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_13    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_14    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_15    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_16    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_17    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_18    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_19    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_20    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_21    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_22    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  11_23    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  12_11    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  12_12    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  12_13    0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  ...      ... ...    ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  \n",
       "  8_10     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_11     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_12     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_13     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_14     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_15     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_16     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_17     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_18     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_19     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_20     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_21     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_22     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_8      0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  8_9      0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_10     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_11     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_12     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_13     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_14     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_15     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_16     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_17     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_18     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_19     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_20     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_21     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_22     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_23     0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  9_9      0.0 ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
       "  \n",
       "  [222 rows x 222 columns]},\n",
       "                    random forest\n",
       " BIKE4EX                      0.0\n",
       " BIKESHARE                    0.0\n",
       " CARRODE                      0.0\n",
       " CARSHARE                     0.0\n",
       " CNTTDHH                      0.0\n",
       " CNTTDTR                      0.0\n",
       " DELIVER                      0.0\n",
       " DISTTOSC17                   0.0\n",
       " DISTTOWK17                   0.0\n",
       " DRVRCNT                      0.0\n",
       " DWELTIME                     0.0\n",
       " DWELTIME_fromwork            0.0\n",
       " GCDWORK                      0.0\n",
       " HHVEHCNT                     0.0\n",
       " HH_ONTD                      0.0\n",
       " HH_ONTD_fromwork             0.0\n",
       " LPACT                        0.0\n",
       " MCUSED                       0.0\n",
       " NBIKETRP                     0.0\n",
       " NOCONG                       0.0\n",
       " NONHHCNT                     0.0\n",
       " NONHHCNT_fromwork            0.0\n",
       " NUMADLT                      0.0\n",
       " NUMONTRP                     0.0\n",
       " NUMONTRP_fromwork            0.0\n",
       " NUMTRANS                     0.0\n",
       " NUMTRANS_fromwork            0.0\n",
       " NWALKTRP                     0.0\n",
       " PTUSED                       0.0\n",
       " PUBTIME                      0.0\n",
       " ...                          ...\n",
       " WRKTRANS_____10              0.0\n",
       " WRKTRANS_____11              0.0\n",
       " WRKTRANS_____13              0.0\n",
       " WRKTRANS_____15              0.0\n",
       " WRKTRANS_____16              0.0\n",
       " WRKTRANS_____17              0.0\n",
       " WRKTRANS_____18              0.0\n",
       " WRKTRANS_____19              0.0\n",
       " WRKTRANS_____2               0.0\n",
       " WRKTRANS_____3               0.0\n",
       " WRKTRANS_____4               0.0\n",
       " WRKTRANS_____5               0.0\n",
       " WRKTRANS_____6               0.0\n",
       " WRKTRANS_____7               0.0\n",
       " WRKTRANS_____8               0.0\n",
       " WRKTRANS_____9               0.0\n",
       " WRKTRANS_____97              0.0\n",
       " WRK_HOME_____-1              0.0\n",
       " WRK_HOME_____-9              0.0\n",
       " WRK_HOME_____1               0.0\n",
       " WRK_HOME_____2               0.0\n",
       " W_CANE_____-1                0.0\n",
       " W_CANE_____1                 0.0\n",
       " W_CRUTCH_____5               0.0\n",
       " W_DOG_____4                  0.0\n",
       " W_NONE_____-1                0.0\n",
       " W_NONE_____0                 0.0\n",
       " W_SCOOTR_____6               0.0\n",
       " W_WLKR_____-1                0.0\n",
       " W_WLKR_____2                 0.0\n",
       " \n",
       " [7433 rows x 1 columns])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2_pipe.score(X_test, y_test, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_steps_fixed = copy.deepcopy(clf.steps[:2])\n",
    "transform_steps_fixed[1][1].fit = types.MethodType(fake_fit, transform_steps_fixed[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pipe = sklearn.pipeline.Pipeline(transform_steps_fixed + [\n",
    "    (\"classification\", MultiEstimator([(\"mlp\", OneLayerNNClassifier(n_epochs=5000, regularization=0.01, dropout=1))]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/handlers.py\", line 633, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/handlers.py\", line 600, in makePickle\n",
      "    d['msg'] = record.getMessage()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-70-5256c43ee47b>\", line 1, in <module>\n",
      "    mlp_pipe.fit(X_train, y_train, classification__sample_weight=weights_train)\n",
      "  File \"/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/rufous/occupancy_detection/t7_travel_demand/src/ml_battery/stacked_estimators.py\", line 59, in fit\n",
      "    log.info(\"opened sessions: \", n_opened_sessions())\n",
      "Message: 'opened sessions: '\n",
      "Arguments: ('0/0',)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('data_transformation', <ml_battery.ml_helpers.CodebookTransformer object at 0x7feb1557ebe0>), ('feature_selection', SelectFromModelPandas(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None...       n_epochs=5000, n_hidden=20, regularization=0.01, trainable=True))],\n",
       "        parallel=False))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pipe.fit(X_train, y_train, classification__sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah(*args, **kwargs):\n",
    "    return None, np.zeros((7405, 7405))\n",
    "permutation_importance.get_score_importances = blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/rufous/occupancy_detection/modules/modules/build/all_of_it/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                    mlp\n",
       " accuracy                                       0.610450\n",
       " weighted f1                                    0.594899\n",
       " weighted precision                             0.588836\n",
       " weighted recall                                0.610450\n",
       " macro f1                                       0.211309\n",
       " macro precision                                0.207744\n",
       " macro recall                                   0.234441\n",
       " log loss                                       2.673084\n",
       " macro mean absolute market share error       391.500974\n",
       " weighted mean absolute market share error      0.407037\n",
       " training time                              11494.988033,\n",
       " {'mlp':               10_10  10_11  10_12  10_13  10_14  10_15         10_16  \\\n",
       "  10_10  2.659389e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_11  1.721172e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_12  9.173025e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_13  4.705944e+06    0.0    0.0    0.0    0.0    0.0  1.528066e+05   \n",
       "  10_14  5.345764e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_15  7.786983e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_16  9.724065e+04    0.0    0.0    0.0    0.0    0.0  1.748616e+07   \n",
       "  10_17  9.543513e+05    0.0    0.0    0.0    0.0    0.0  1.787489e+06   \n",
       "  10_18  4.615266e+05    0.0    0.0    0.0    0.0    0.0  1.023123e+05   \n",
       "  10_19  4.652575e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_20  6.645937e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_21  2.017081e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_22  2.832456e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  10_23  5.464431e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_11  3.620878e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_12  1.074647e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_13  8.002094e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_14  7.064374e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_15  1.998820e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_16  2.466374e+04    0.0    0.0    0.0    0.0    0.0  1.449997e+07   \n",
       "  11_17  8.386541e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_18  8.324444e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_19  1.456470e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_20  1.026679e+07    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_21  1.619427e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_22  8.480398e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  11_23  1.641409e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  12_11  0.000000e+00    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  12_12  5.742662e+04    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  12_13  8.517925e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  ...             ...    ...    ...    ...    ...    ...           ...   \n",
       "  8_10   1.125169e+07    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_11   4.201138e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_12   9.120200e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_13   1.311631e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_14   0.000000e+00    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_15   2.455621e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_16   0.000000e+00    0.0    0.0    0.0    0.0    0.0  8.142409e+04   \n",
       "  8_17   1.295282e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_18   4.949872e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_19   0.000000e+00    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_20   1.070891e+07    0.0    0.0    0.0    0.0    0.0  2.540188e+06   \n",
       "  8_21   6.065587e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_22   5.929305e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_8    2.617653e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  8_9    1.098201e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_10   1.182769e+07    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_11   1.675250e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_12   1.522722e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_13   3.018908e+06    0.0    0.0    0.0    0.0    0.0  7.578550e+06   \n",
       "  9_14   8.947316e+04    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_15   8.116455e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_16   1.045955e+05    0.0    0.0    0.0    0.0    0.0  2.922631e+05   \n",
       "  9_17   4.766157e+04    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_18   2.526162e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_19   3.389398e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_20   5.822538e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_21   2.415289e+05    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_22   9.971032e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_23   0.000000e+00    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  9_9    1.322717e+06    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "  \n",
       "                10_17         10_18         10_19 ...   9_15          9_16  \\\n",
       "  10_10  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_11  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_12  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_13  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_14  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  3.641872e+05   \n",
       "  10_15  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_16  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  1.913960e+06   \n",
       "  10_17  1.484301e+07  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_18  2.848026e+05  1.480815e+07  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_19  8.656094e+05  6.463428e+05  1.265622e+07 ...    0.0  0.000000e+00   \n",
       "  10_20  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_21  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_22  1.887561e+05  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  10_23  1.291827e+06  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_11  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_12  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_13  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_14  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_15  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_16  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  2.890074e+04   \n",
       "  11_17  2.023815e+06  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_18  0.000000e+00  2.508184e+06  1.131428e+04 ...    0.0  0.000000e+00   \n",
       "  11_19  0.000000e+00  0.000000e+00  1.145304e+05 ...    0.0  0.000000e+00   \n",
       "  11_20  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_21  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_22  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  11_23  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  12_11  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  12_12  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  12_13  0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  ...             ...           ...           ... ...    ...           ...   \n",
       "  8_10   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_11   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_12   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_13   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_14   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_15   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_16   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  1.081102e+07   \n",
       "  8_17   1.945195e+05  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_18   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_19   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_20   0.000000e+00  0.000000e+00  1.000284e+06 ...    0.0  0.000000e+00   \n",
       "  8_21   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  8.683886e+05   \n",
       "  8_22   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_8    0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  8_9    0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_10   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_11   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_12   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_13   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  4.668955e+05   \n",
       "  9_14   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  2.841705e+05   \n",
       "  9_15   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  7.286132e+05   \n",
       "  9_16   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  2.379101e+07   \n",
       "  9_17   2.569675e+06  1.754986e+06  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_18   1.974083e+05  2.650452e+06  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_19   0.000000e+00  0.000000e+00  4.365042e+05 ...    0.0  0.000000e+00   \n",
       "  9_20   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_21   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  1.603957e+06   \n",
       "  9_22   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_23   0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  9_9    0.000000e+00  0.000000e+00  0.000000e+00 ...    0.0  0.000000e+00   \n",
       "  \n",
       "                 9_17          9_18          9_19          9_20          9_21  \\\n",
       "  10_10  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_12  1.866892e+05  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_14  1.992605e+05  8.833371e+06  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_15  6.648848e+05  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_16  0.000000e+00  0.000000e+00  5.383100e+05  0.000000e+00  0.000000e+00   \n",
       "  10_17  1.492138e+06  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_18  0.000000e+00  9.383863e+05  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  10_19  0.000000e+00  0.000000e+00  7.159798e+06  0.000000e+00  0.000000e+00   \n",
       "  10_20  0.000000e+00  0.000000e+00  0.000000e+00  2.627595e+05  0.000000e+00   \n",
       "  10_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  4.209685e+05   \n",
       "  10_22  0.000000e+00  0.000000e+00  0.000000e+00  9.301282e+04  0.000000e+00   \n",
       "  10_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_14  1.296382e+06  1.443492e+04  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_16  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_17  1.688658e+06  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_18  3.429027e+04  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_19  0.000000e+00  0.000000e+00  1.202749e+05  0.000000e+00  0.000000e+00   \n",
       "  11_20  0.000000e+00  0.000000e+00  0.000000e+00  5.846577e+05  0.000000e+00   \n",
       "  11_21  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_22  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  11_23  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_11  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_12  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  12_13  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  ...             ...           ...           ...           ...           ...   \n",
       "  8_10   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_11   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_12   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_13   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_14   4.639821e+05  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_15   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_16   6.892182e+04  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_17   2.055130e+07  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_18   0.000000e+00  8.916540e+06  1.009867e+05  3.093091e+05  0.000000e+00   \n",
       "  8_19   0.000000e+00  0.000000e+00  8.548496e+06  0.000000e+00  0.000000e+00   \n",
       "  8_20   0.000000e+00  0.000000e+00  0.000000e+00  1.187026e+05  0.000000e+00   \n",
       "  8_21   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.076728e+05   \n",
       "  8_22   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_8    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  8_9    0.000000e+00  0.000000e+00  0.000000e+00  1.004993e+05  0.000000e+00   \n",
       "  9_10   7.595009e+03  4.783177e+06  0.000000e+00  1.574652e+04  7.956785e+05   \n",
       "  9_11   4.526109e+05  0.000000e+00  9.695664e+05  2.373324e+04  0.000000e+00   \n",
       "  9_12   0.000000e+00  3.209789e+05  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_13   0.000000e+00  2.856546e+06  3.962198e+05  0.000000e+00  0.000000e+00   \n",
       "  9_14   0.000000e+00  0.000000e+00  0.000000e+00  1.437822e+04  0.000000e+00   \n",
       "  9_15   4.896358e+06  0.000000e+00  0.000000e+00  3.247179e+04  0.000000e+00   \n",
       "  9_16   2.093344e+06  0.000000e+00  5.217421e+04  0.000000e+00  0.000000e+00   \n",
       "  9_17   3.319278e+07  1.362369e+05  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_18   5.920922e+05  3.649904e+07  2.580718e+05  0.000000e+00  0.000000e+00   \n",
       "  9_19   0.000000e+00  1.810769e+05  1.879507e+07  0.000000e+00  2.252489e+05   \n",
       "  9_20   3.833131e+06  1.260899e+06  2.265954e+06  1.832284e+07  9.632546e+04   \n",
       "  9_21   0.000000e+00  9.443006e+04  0.000000e+00  4.157403e+04  6.048260e+06   \n",
       "  9_22   0.000000e+00  9.340323e+05  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_23   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  9_9    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "  \n",
       "                 9_22  9_23  9_9  \n",
       "  10_10  0.000000e+00   0.0  0.0  \n",
       "  10_11  0.000000e+00   0.0  0.0  \n",
       "  10_12  0.000000e+00   0.0  0.0  \n",
       "  10_13  0.000000e+00   0.0  0.0  \n",
       "  10_14  0.000000e+00   0.0  0.0  \n",
       "  10_15  0.000000e+00   0.0  0.0  \n",
       "  10_16  0.000000e+00   0.0  0.0  \n",
       "  10_17  0.000000e+00   0.0  0.0  \n",
       "  10_18  0.000000e+00   0.0  0.0  \n",
       "  10_19  0.000000e+00   0.0  0.0  \n",
       "  10_20  0.000000e+00   0.0  0.0  \n",
       "  10_21  0.000000e+00   0.0  0.0  \n",
       "  10_22  0.000000e+00   0.0  0.0  \n",
       "  10_23  0.000000e+00   0.0  0.0  \n",
       "  11_11  0.000000e+00   0.0  0.0  \n",
       "  11_12  0.000000e+00   0.0  0.0  \n",
       "  11_13  0.000000e+00   0.0  0.0  \n",
       "  11_14  0.000000e+00   0.0  0.0  \n",
       "  11_15  0.000000e+00   0.0  0.0  \n",
       "  11_16  0.000000e+00   0.0  0.0  \n",
       "  11_17  0.000000e+00   0.0  0.0  \n",
       "  11_18  0.000000e+00   0.0  0.0  \n",
       "  11_19  0.000000e+00   0.0  0.0  \n",
       "  11_20  0.000000e+00   0.0  0.0  \n",
       "  11_21  0.000000e+00   0.0  0.0  \n",
       "  11_22  0.000000e+00   0.0  0.0  \n",
       "  11_23  0.000000e+00   0.0  0.0  \n",
       "  12_11  0.000000e+00   0.0  0.0  \n",
       "  12_12  0.000000e+00   0.0  0.0  \n",
       "  12_13  0.000000e+00   0.0  0.0  \n",
       "  ...             ...   ...  ...  \n",
       "  8_10   0.000000e+00   0.0  0.0  \n",
       "  8_11   0.000000e+00   0.0  0.0  \n",
       "  8_12   0.000000e+00   0.0  0.0  \n",
       "  8_13   0.000000e+00   0.0  0.0  \n",
       "  8_14   0.000000e+00   0.0  0.0  \n",
       "  8_15   0.000000e+00   0.0  0.0  \n",
       "  8_16   0.000000e+00   0.0  0.0  \n",
       "  8_17   0.000000e+00   0.0  0.0  \n",
       "  8_18   0.000000e+00   0.0  0.0  \n",
       "  8_19   0.000000e+00   0.0  0.0  \n",
       "  8_20   0.000000e+00   0.0  0.0  \n",
       "  8_21   0.000000e+00   0.0  0.0  \n",
       "  8_22   0.000000e+00   0.0  0.0  \n",
       "  8_8    0.000000e+00   0.0  0.0  \n",
       "  8_9    0.000000e+00   0.0  0.0  \n",
       "  9_10   0.000000e+00   0.0  0.0  \n",
       "  9_11   0.000000e+00   0.0  0.0  \n",
       "  9_12   0.000000e+00   0.0  0.0  \n",
       "  9_13   0.000000e+00   0.0  0.0  \n",
       "  9_14   0.000000e+00   0.0  0.0  \n",
       "  9_15   0.000000e+00   0.0  0.0  \n",
       "  9_16   0.000000e+00   0.0  0.0  \n",
       "  9_17   0.000000e+00   0.0  0.0  \n",
       "  9_18   0.000000e+00   0.0  0.0  \n",
       "  9_19   0.000000e+00   0.0  0.0  \n",
       "  9_20   0.000000e+00   0.0  0.0  \n",
       "  9_21   0.000000e+00   0.0  0.0  \n",
       "  9_22   1.761086e+06   0.0  0.0  \n",
       "  9_23   0.000000e+00   0.0  0.0  \n",
       "  9_9    0.000000e+00   0.0  0.0  \n",
       "  \n",
       "  [222 rows x 222 columns]},\n",
       "                    mlp\n",
       " BIKE4EX            0.0\n",
       " BIKESHARE          0.0\n",
       " CARRODE            0.0\n",
       " CARSHARE           0.0\n",
       " CNTTDHH            0.0\n",
       " CNTTDTR            0.0\n",
       " DELIVER            0.0\n",
       " DISTTOSC17         0.0\n",
       " DISTTOWK17         0.0\n",
       " DRVRCNT            0.0\n",
       " DWELTIME           0.0\n",
       " DWELTIME_fromwork  0.0\n",
       " GCDWORK            0.0\n",
       " HHVEHCNT           0.0\n",
       " HH_ONTD            0.0\n",
       " HH_ONTD_fromwork   0.0\n",
       " LPACT              0.0\n",
       " MCUSED             0.0\n",
       " NBIKETRP           0.0\n",
       " NOCONG             0.0\n",
       " NONHHCNT           0.0\n",
       " NONHHCNT_fromwork  0.0\n",
       " NUMADLT            0.0\n",
       " NUMONTRP           0.0\n",
       " NUMONTRP_fromwork  0.0\n",
       " NUMTRANS           0.0\n",
       " NUMTRANS_fromwork  0.0\n",
       " NWALKTRP           0.0\n",
       " PTUSED             0.0\n",
       " PUBTIME            0.0\n",
       " ...                ...\n",
       " WRKTRANS_____10    0.0\n",
       " WRKTRANS_____11    0.0\n",
       " WRKTRANS_____13    0.0\n",
       " WRKTRANS_____15    0.0\n",
       " WRKTRANS_____16    0.0\n",
       " WRKTRANS_____17    0.0\n",
       " WRKTRANS_____19    0.0\n",
       " WRKTRANS_____2     0.0\n",
       " WRKTRANS_____3     0.0\n",
       " WRKTRANS_____4     0.0\n",
       " WRKTRANS_____5     0.0\n",
       " WRKTRANS_____6     0.0\n",
       " WRKTRANS_____8     0.0\n",
       " WRKTRANS_____97    0.0\n",
       " WRK_HOME_____-1    0.0\n",
       " WRK_HOME_____-9    0.0\n",
       " WRK_HOME_____1     0.0\n",
       " WRK_HOME_____2     0.0\n",
       " W_CANE_____-1      0.0\n",
       " W_CANE_____1       0.0\n",
       " W_CHAIR_____-1     0.0\n",
       " W_CRUTCH_____-1    0.0\n",
       " W_DOG_____4        0.0\n",
       " W_MTRCHR_____-1    0.0\n",
       " W_NONE_____-1      0.0\n",
       " W_NONE_____0       0.0\n",
       " W_SCOOTR_____-1    0.0\n",
       " W_WHCANE_____-1    0.0\n",
       " W_WLKR_____-1      0.0\n",
       " W_WLKR_____2       0.0\n",
       " \n",
       " [7405 rows x 1 columns])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pipe.score(X_test, y_test, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
